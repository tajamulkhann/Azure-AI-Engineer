{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df5ed8e",
   "metadata": {},
   "source": [
    "## Evaluating in an Azure OpenAI RAG System\n",
    "\n",
    "Retriever â€“ e.g., Azure Cognitive Search or a vector DB.\n",
    "\n",
    "Generator â€“ e.g., Azure OpenAI gpt-4o-mini.\n",
    "\n",
    "For each query:\n",
    "\n",
    "- Run the retriever â†’ get top-k docs (contexts)\n",
    "\n",
    "- Run the generator â†’ get the LLM answer (answer)\n",
    "\n",
    "- Collect together with the gold answer(s)\n",
    "\n",
    "Then run RAGAS evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90b44df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate, EvaluationDataset\n",
    "\n",
    "eval_dataset = EvaluationDataset.from_list([\n",
    "    {\n",
    "        \"question\": \"Who is the CEO of Tesla?\",\n",
    "        \"answer\": \"Elon Musk is the CEO of Tesla.\",\n",
    "        \"contexts\": [\"Tesla, Inc. is led by CEO Elon Musk...\"],\n",
    "        \"ground_truths\": [\"Elon Musk\"]\n",
    "    }\n",
    "])\n",
    "\n",
    "results = evaluate(dataset=eval_dataset, llm=evaluator_llm)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2243b4",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "{\n",
    "  \"hit_rate\": 1.0,\n",
    "  \"mrr\": 0.9,\n",
    "  \"answer_relevancy\": 0.98,\n",
    "  \"faithfulness\": 0.95\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634eb625",
   "metadata": {},
   "source": [
    "| Metric               | What it measures                         | Example insight                      |\n",
    "| :------------------- | :--------------------------------------- | :----------------------------------- |\n",
    "| **Hit Rate**         | Retriever found at least one correct doc | If low â†’ retriever misses key info   |\n",
    "| **MRR**              | How early the correct doc appears        | If low â†’ retriever ranks docs poorly |\n",
    "| **Answer Relevancy** | Whether the answer answers the question  | If low â†’ LLM not following query     |\n",
    "| **Faithfulness**     | Whether answer is supported by context   | If low â†’ hallucination issue         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef1b9c",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Key Idea\n",
    "\n",
    "The benchmark you compare against = gold answers (and sometimes gold docs) from your test dataset.\n",
    "\n",
    "Retriever metrics (Hit Rate, MRR) compare retrieved docs vs gold answers.\n",
    "\n",
    "Generator metrics (Relevancy, Faithfulness) compare generated answers vs gold answers and/or contexts.\n",
    "\n",
    "### âœ… In short:\n",
    "When you evaluate your Azure OpenAI RAG system:\n",
    "\n",
    "- Hit Rate & MRR â†’ How well your retriever finds and ranks relevant docs.\n",
    "\n",
    "- Answer Relevancy â†’ Whether your generated answer actually answers the question.\n",
    "\n",
    "- Faithfulness â†’ Whether your answer is supported by retrieved context (not hallucinated).\n",
    "\n",
    "Together, these metrics let you pinpoint weaknesses â€” whether to improve retrieval quality, LLM grounding, or both."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
