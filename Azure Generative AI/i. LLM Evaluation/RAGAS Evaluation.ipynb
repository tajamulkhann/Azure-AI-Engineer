{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "775a2d21",
      "metadata": {},
      "source": [
        "## RAGAS Evaluation\n",
        "\n",
        "RAGAS = Retrieval-Augmented Generation Assessment Suite (an evaluation framework).\n",
        "\n",
        "Itâ€™s designed to evaluate retrieval-augmented generation (RAG) systems.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6568ed0",
      "metadata": {},
      "source": [
        "### 1 - Install dependencies\n",
        "\n",
        "If you haven't installed the required packages, run the cell below once. It's commented out so you don't accidentally reinstall packages every run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0160671f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install --upgrade pip\n",
        "# !pip install ragas langchain openai python-dotenv\n",
        "print('Run the pip install commands above if you need to install dependencies')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a16ddc9c",
      "metadata": {},
      "source": [
        "### 2 - Imports & environment\n",
        "\n",
        "This cell loads environment variables (from a `.env` file if present) and checks that `ragas` is importable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cddb275d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tajamulkhan/Desktop/Azure-AI-Engineering/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python and ragas imported OK\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # reads .env if present\n",
        "\n",
        "# Basic check for ragas\n",
        "try:\n",
        "    import ragas\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Missing 'ragas'. Install it with: pip install ragas\") from e\n",
        "\n",
        "print('Python and ragas imported OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c18b4d34",
      "metadata": {},
      "source": [
        "### 3 - Configure LLM (Azure or OpenAI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6fe75509",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using ChatOpenAI model: gpt-4o-mini\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/f0/hbgqnpxj3y120xjbg_znbfpc0000gn/T/ipykernel_38091/881815035.py:18: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.0, model=model_name)\n"
          ]
        }
      ],
      "source": [
        "use_azure = bool(os.getenv('AZURE_API_KEY') and os.getenv('AZURE_DEPLOYMENT_NAME'))\n",
        "\n",
        "if use_azure:\n",
        "    from langchain.chat_models import AzureChatOpenAI\n",
        "    azure_endpoint = os.getenv('AZURE_ENDPOINT')\n",
        "    azure_key = os.getenv('AZURE_API_KEY')\n",
        "    deployment_name = os.getenv('AZURE_DEPLOYMENT_NAME')\n",
        "    if not (azure_endpoint and azure_key and deployment_name):\n",
        "        raise RuntimeError('Set AZURE_ENDPOINT, AZURE_API_KEY and AZURE_DEPLOYMENT_NAME for Azure')\n",
        "    llm = AzureChatOpenAI(azure_deployment=deployment_name, api_key=azure_key, azure_endpoint=azure_endpoint, temperature=0.0)\n",
        "    print('Using AzureChatOpenAI with deployment:', deployment_name)\n",
        "else:\n",
        "    from langchain.chat_models import ChatOpenAI\n",
        "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "    if not openai_api_key:\n",
        "        raise RuntimeError('Set OPENAI_API_KEY for non-Azure usage')\n",
        "    model_name = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')\n",
        "    llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.0, model=model_name)\n",
        "    print('Using ChatOpenAI model:', model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cc51700",
      "metadata": {},
      "source": [
        "### 4 - Wrap LangChain LLM for Ragas\n",
        "\n",
        "Ragas expects a wrapper around LangChain LLMs. This cell wraps the `llm` created above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "88359f99",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrapped LangChain LLM for Ragas\n"
          ]
        }
      ],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "evaluator_llm = LangchainLLMWrapper(llm)\n",
        "print('Wrapped LangChain LLM for Ragas')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8178a87",
      "metadata": {},
      "source": [
        "### 5 - Prepare evaluation samples\n",
        "\n",
        "Replace the `samples` list below with your system outputs. Each sample should be a dict with (commonly) these keys: `question`, `answer` (the system's response), `contexts` (list of retrieved documents/strings), and optionally `ground_truths` (list of correct answers for metrics that require them)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bd9c6da8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constructed EvaluationDataset with 2 samples\n",
            "Available features: []\n"
          ]
        }
      ],
      "source": [
        "samples = [\n",
        "    {\n",
        "        \"question\": \"Where was Albert Einstein born and when?\",\n",
        "        \"answer\": \"Albert Einstein was born in Ulm, Germany on 14 March 1879.\",\n",
        "        \"contexts\": [\n",
        "            \"Albert Einstein (born 14 March 1879 in Ulm) was a German-born theoretical physicist.\"\n",
        "        ],\n",
        "        \"ground_truths\": [\"Ulm, Germany on 14 March 1879\"]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the capital of France?\",\n",
        "        \"answer\": \"Paris is the capital of France.\",\n",
        "        \"contexts\": [\"Paris is the capital and most populous city of France.\"],\n",
        "        \"ground_truths\": [\"Paris\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "from ragas import EvaluationDataset\n",
        "\n",
        "eval_dataset = EvaluationDataset.from_list(samples)\n",
        "print('Constructed EvaluationDataset with', len(eval_dataset.samples), 'samples')\n",
        "try:\n",
        "    print('Available features:', eval_dataset.features())\n",
        "except Exception:\n",
        "    print('Could not call eval_dataset.features(); available attributes on dataset object:')\n",
        "    print([a for a in dir(eval_dataset) if not a.startswith('_')][:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f38c26ec",
      "metadata": {},
      "source": [
        "### 6 - Run evaluation (default metric set)\n",
        "\n",
        "This runs `evaluate(dataset, llm=...)` letting Ragas choose a reasonable default metric set for your dataset/version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "621558a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluate(dataset, llm=...) with default metric set...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The metric [answer_relevancy] that is used requires the following additional columns ['user_input', 'response'] to be present in the dataset.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mRunning evaluate(dataset, llm=...) with default metric set...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m results_default = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluator_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDefault evaluation results:\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(results_default)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Azure-AI-Engineering/venv/lib/python3.13/site-packages/ragas/_analytics.py:277\u001b[39m, in \u001b[36mtrack_was_completed.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    276\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Azure-AI-Engineering/venv/lib/python3.13/site-packages/ragas/evaluation.py:195\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar, return_executor)\u001b[39m\n\u001b[32m    192\u001b[39m     dataset = EvaluationDataset.from_list(dataset.to_list())\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, EvaluationDataset):\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     \u001b[43mvalidate_required_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m     validate_supported_metrics(dataset, metrics)\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# set the llm and embeddings\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Azure-AI-Engineering/venv/lib/python3.13/site-packages/ragas/validation.py:63\u001b[39m, in \u001b[36mvalidate_required_columns\u001b[39m\u001b[34m(ds, metrics)\u001b[39m\n\u001b[32m     61\u001b[39m available_columns = \u001b[38;5;28mset\u001b[39m(ds.features())\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m required_columns.issubset(available_columns):\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     64\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe metric [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] that is used requires the following \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     65\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33madditional columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(required_columns\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mavailable_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto be present in the dataset.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: The metric [answer_relevancy] that is used requires the following additional columns ['user_input', 'response'] to be present in the dataset."
          ]
        }
      ],
      "source": [
        "from ragas import evaluate\n",
        "\n",
        "print('Running evaluate(dataset, llm=...) with default metric set...')\n",
        "results_default = evaluate(dataset=eval_dataset, llm=evaluator_llm, show_progress=False)\n",
        "\n",
        "print('\\nDefault evaluation results:')\n",
        "print(results_default)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "050436b5",
      "metadata": {},
      "source": [
        "### 7 - Optional: explicit metric run (robust across Ragas versions)\n",
        "\n",
        "Ragas changes metric class names across versions. This cell tries several common metric names dynamically and runs them if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "05624ca2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting explicit metrics: ['Faithfulness', 'AnswerRelevancy', 'ContextRecall', 'ContextPrecision', 'LLMContextRecall', 'FactualCorrectness', 'SemanticSimilarity']\n",
            "Explicit metrics run failed: The metric [faithfulness] that is used requires the following additional columns ['retrieved_contexts', 'user_input', 'response'] to be present in the dataset.\n"
          ]
        }
      ],
      "source": [
        "from ragas import metrics as ragas_metrics_module\n",
        "\n",
        "metric_candidates = [\n",
        "    \"Faithfulness\", \"AnswerRelevancy\", \"ContextRecall\", \"ContextPrecision\",\n",
        "    \"LLMContextRecall\", \"FactualCorrectness\", \"SemanticSimilarity\"\n",
        "]\n",
        "\n",
        "metric_instances = []\n",
        "for mn in metric_candidates:\n",
        "    cls = getattr(ragas_metrics_module, mn, None)\n",
        "    if cls is not None:\n",
        "        try:\n",
        "            metric_instances.append(cls())\n",
        "        except Exception:\n",
        "            metric_instances.append(cls)\n",
        "\n",
        "if metric_instances:\n",
        "    try:\n",
        "        print('Attempting explicit metrics:', [getattr(m, '__class__', m).__name__ for m in metric_instances])\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        results_explicit = evaluate(dataset=eval_dataset, metrics=metric_instances, llm=evaluator_llm, show_progress=False)\n",
        "        print('Explicit metrics results:')\n",
        "        print(results_explicit)\n",
        "    except Exception as e:\n",
        "        print('Explicit metrics run failed:', e)\n",
        "else:\n",
        "    print('No compatible explicit metric classes found on this ragas installation; stick to defaults.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a68235d2",
      "metadata": {},
      "source": [
        "### 8 - Save & Export results (optional)\n",
        "\n",
        "This cell saves the dataset samples to CSV and the default evaluation results to JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d41a07",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import pandas as pd\n",
        "    df = eval_dataset.to_pandas()\n",
        "    df.to_csv('ragas_eval_dataset_samples.csv', index=False)\n",
        "    print('Saved ragas_eval_dataset_samples.csv')\n",
        "except Exception as e:\n",
        "    print('Could not save sample CSV:', e)\n",
        "\n",
        "import json\n",
        "with open('ragas_eval_results.json', 'w') as f:\n",
        "    json.dump(results_default, f, indent=2)\n",
        "print('Saved ragas_eval_results.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83f46f00",
      "metadata": {},
      "source": [
        "### Notes & Troubleshooting\n",
        "\n",
        "- If you hit `AttributeError` regarding metric class names, run `dir(ragas.metrics)` to inspect available metric classes and adapt the list in Cell 7.\n",
        "- If your original notebook used different column names (e.g., `retrieved_contexts` or `response`), map them to `contexts` / `answer` / `ground_truths` when building `samples` above.\n",
        "- For Azure: make sure `AZURE_ENDPOINT` ends with `/` and matches the format from your Azure portal.\n",
        "- If you want me to patch your exact uploaded `.ipynb` and return the updated file, tell me and I'll create a notebook file in /mnt/data for download."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "025b2c4d",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
