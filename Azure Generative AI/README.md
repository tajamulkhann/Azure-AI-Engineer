# ðŸš€ Azure Generative AI + RAG (Retrieval-Augmented Generation)

## ðŸ”¹ Overview
This repository demonstrates building a **Retrieval-Augmented Generation (RAG)** system on Azure.  
It combines **Azure OpenAI Service** for LLM generation with **Azure Cognitive Search** for semantic retrieval.  
RAG allows LLMs to answer queries using **external knowledge** instead of relying solely on pre-trained knowledge.

---

## ðŸ”¹ Prerequisites for RAG with Azure AI

### 1ï¸âƒ£ Azure Resources
- **Storage Account**: To store your documents and data.  
- **Embedding Deployment**: Azure OpenAI deployment for generating embeddings.  
- **Chats Deployment**: Azure OpenAI deployment for chat generation.  
- **Azure AI Search Service**: Semantic search over your documents.

### 2ï¸âƒ£ Using Chat Deployment
- **Upload documents** to storage.  
- **Ingestion â†’ Preprocessing â†’ Indexing** into Azure Cognitive Search.  

> Example services and datastore naming conventions:  
- `acasdatastore` â†’ Azure data storage  
- `embeddingacasrag` â†’ Embedding deployment  
- `chatmodelrag` â†’ Chat model deployment  
- `azureaisearchrag` â†’ Search index/service  

---

## ðŸ”¹ Workflow

### Step 1: Data Ingestion
- Load PDFs, CSVs, text files, etc.  
- Split into smaller chunks (paragraph â†’ sentence â†’ word) using `RecursiveCharacterTextSplitter`.

### Step 2: Embedding & Storage
- Convert text chunks into **vector embeddings** using Azure OpenAI or Hugging Face.  
- Store vectors in **Azure Cognitive Search** (vector DB).  

#### Index Types
| Index Type | Description | Use Case |
|------------|-------------|----------|
| **Flat**   | Linear scan | Small datasets, exact search |
| **HNSW**   | Graph-based approximate search | Large datasets, fast search |
| **IVF**    | Clustering + inverted file | Very large datasets |
| **PQ**     | Quantized vectors | Memory-efficient retrieval |

---

### Step 3: Azure OpenAI Client Setup
```python
from azure.ai.openai import OpenAIClient
from azure.core.credentials import AzureKeyCredential

# Initialize client
client = OpenAIClient(
    endpoint="https://<your-resource>.openai.azure.com/",
    credential=AzureKeyCredential("<your-api-key>"),
    api_version="2023-07-01-preview"
)

# Example usage
response = client.chat_completions.create(
    deployment_name="<your-chat-deployment>",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain Azure Cognitive Search."}
    ]
)
print(response.choices[0].message.content)
```

### Step 4: Retrieval

- User query is converted into embeddings.

- Retrieve top-k similar documents from Azure Cognitive Search index.

### Step 5: Augmented Generation (RAG Chain)
```python
from langchain.schema import RunnablePassthrough, StrOutputParser

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)
```

### Process:

- Retriever: Fetch relevant vectors/documents from search.

- Format Docs: Prepare retrieved docs as context.

- Prompt: Combine query + context for LLM.

- Model: Azure OpenAI generates context-aware response.

- Output Parser: Return clean string output.

### ðŸ”¹ Key Takeaways

- Azure OpenAI + Cognitive Search enables semantic search + intelligent generation.

- Embeddings are crucial for accurate retrieval.

- Index type affects speed and accuracy: HNSW recommended for large datasets.

- Use unique IDs for each document vector in the datastore.

- Fully modular: add new documents or change model deployments easily.
